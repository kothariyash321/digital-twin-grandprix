# Purdue Grand Prix Digital Twin & Real-Time Analytics Platform

This repository contains the complete codebase and documentation for the  Grand Prix digital twin project, a real-time data analytics and simulation platform developed in collaboration with the Purdue Grand Prix go-kart racing team.

## Table of Contents
- [Project Overview](#project-overview)
- [Live Demo](#live-demo)
- [Features](#features)
- [System Architecture](#system-architecture)
- [Technology Stack](#technology-stack)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
  - [1. Start Services](#1-start-services)
  - [2. Run an Application](#2-run-an-application)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [License](#license)

## Project Overview
The  Grand Prix digital twin project is a full-stack data analytics solution designed to provide real-time insights into vehicle performance, driver behavior, and predictive maintenance for the Grand Prix go-kart racing team. By creating a digital replica of the race car, the team can analyze telemetry data, predict component failures, and optimize racing strategies.

This project was a cross-disciplinary collaboration between students from data science, computer science, and mechanical engineering, following agile development principles to deliver a comprehensive and impactful solution.

## Go-Kart Sensors

The go-kart is equipped with multiple sensors that collect real-time telemetry data during races.
![Sensor Description](https://cdn1.genspark.ai/user-upload-image/5_generated/0f657beb-e5c5-4b72-9c83-7786f7a3d825_wm)

*Technical illustration of racing go-kart with sensors highlighted*


## Live Demo
![Dashboard](https://cdn1.genspark.ai/user-upload-image/5_generated/8f96f568-eb95-450d-a2f3-13d3fd5c1eaa_wm)

*Caption: The UI displays live telemetry and critical alerts generated by the streaming pipeline.*

## Features
- **Real-time Data Ingestion:** Collects and processes data from 15+ sensors at 100Hz via Kafka.
- **Predictive Maintenance:** AI-driven models to predict component failures with 85% accuracy.
- **Optimal Racing Line Prediction:** ML models to suggest optimal racing strategies based on track and vehicle data.
- **Real-time System Monitoring:** Continuously monitors vehicle parameters and triggers alerts for safety-critical events.
- **Web-Based Digital Twin:** An interactive dashboard for visualizing live telemetry and historical data.

## System Architecture
The system is designed as a distributed, event-driven architecture. For a detailed diagram and explanation, see the [Architecture Documentation](./docs/architecture.md).


![Architectre](https://cdn1.genspark.ai/user-upload-image/5_generated/5f62195e-5220-4407-a772-2f26ebf8e019_wm)



## Technology Stack
- **Data Ingestion:** Apache Kafka
- **Data Processing:** Apache Spark (Streaming & Batch)
- **Data Storage:** MongoDB
- **Web Application:** Flask & D3.js
- **Containerization:** Docker

## Getting Started

### Prerequisites
- [Docker](https://www.docker.com/get-started) and Docker Compose
- Python 3.8+ (for running scripts outside of Docker)
- At least 8GB of RAM allocated to Docker is recommended.

### Installation
1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-username/PurdueGP-DigitalTwin.git](https://github.com/your-username/PurdueGP-DigitalTwin.git)
    cd PurdueGP-DigitalTwin
    ```
2.  **Set up the environment:**
    A setup script is provided to create a Python virtual environment and install dependencies.
    ```bash
    bash scripts/setup-environment.sh
    ```

## Usage
The entire application stack can be launched using Docker Compose.

### 1. Start Services
This command will start Kafka, Zookeeper, MongoDB, and Spark containers in the background.
```bash
docker-compose up -d
```
You can check the status of the containers with `docker-compose ps`.

### 2. Run an Application
Open separate terminal windows to run the different components of the application.

- **Terminal 1: Start the Data Producer (Simulated Sensors)**
  ```bash
  source venv/bin/activate
  python src/data_collection/kafka_producer.py
  ```

- **Terminal 2: Start the Spark Streaming Job**
  ```bash
  # This command submits the streaming application to the Spark master running in Docker
  docker-compose exec spark-master /opt/bitnami/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 \
    /app/src/stream_processing/spark_streaming.py
  ```

- **Terminal 3: Start the Flask Web Application**
  ```bash
  source venv/bin/activate
  python src/web/app.py
  ```

- **Access the Dashboard:** Open your web browser and go to `http://localhost:5000`.

## Project Structure
The repository is organized into a modular `src` directory, with supporting documentation, scripts, and configuration at the root level.
```
digital-twin-grandprix/
├── src/
│   ├── analytics/
│   ├── config/
│   ├── data_collection/
│   ├── ml/
│   ├── storage/
│   ├── stream_processing/
│   └── web/
├── docs/
├── scripts/
├── data/
├── tests/
├── .gitignore
├── docker-compose.yml
├── README.md
└── requirements.txt
```

## Contributing
We welcome contributions! Please fork the repository and create a pull request with your changes.


```docker-compose
# ==============================================================================
# File: docker-compose.yml
# Description: Defines and runs the multi-container application stack.
# ==============================================================================
version: '3.8'

services:
  zookeeper:
    image: 'bitnami/zookeeper:3.8'
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: 'bitnami/kafka:3.1'
    ports:
      - '9092:9092'
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - ALLOW_PLAINTEXT_LISTENER=yes
    depends_on:
      - zookeeper

  mongodb:
    image: 'mongo:5.0'
    ports:
      - '27017:27017'
    volumes:
      - 'mongodb_data:/data/db'

  spark-master:
    image: 'bitnami/spark:3'
    ports:
      - '8080:8080' # Spark Master Web UI
      - '7077:7077' # Spark Master Port
    environment:
      - SPARK_MODE=master
    volumes:
      - './src:/app/src' # Mount our app source code

  spark-worker:
    image: 'bitnami/spark:3'
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - './src:/app/src' # Mount our app source code
    depends_on:
      - spark-master

volumes:
  mongodb_data:

```text
# ==============================================================================
# File: requirements.txt
# ==============================================================================
# For Flask Web App
Flask
pymongo
PyYAML

# For Kafka Producer
kafka-python

# For Spark Analytics & ML (to be run via spark-submit)
pyspark==3.2.1

# For running scripts
pyyaml

```text
# ==============================================================================
# File: .gitignore
# ==============================================================================
# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
env/
venv/
*.egg-info/
.ipynb_checkpoints

# IDE / OS files
.idea/
.vscode/
.DS_Store
*.swp

# Build artifacts
build/
dist/
target/

# Logs
*.log

```bash
# ==============================================================================
# File: scripts/start-services.sh
# Description: A helper script to start the core infrastructure.
# ==============================================================================
#!/bin/bash
echo "Starting core infrastructure (Kafka, MongoDB, Spark)..."
docker-compose up -d
echo ""
echo "Services started. Use 'docker-compose ps' to check status."
echo "Use 'docker-compose logs -f <service_name>' to view logs."

```bash
# ==============================================================================
# File: scripts/setup-environment.sh
# Description: Sets up a local Python virtual environment.
# ==============================================================================
#!/bin/bash
echo "Creating Python virtual environment in './venv'..."
python3 -m venv venv

echo "Activating virtual environment..."
source venv/bin/activate

echo "Installing required packages from requirements.txt..."
pip install -r requirements.txt

echo ""
echo "Setup complete. To activate the environment in the future, run:"
echo "source venv/bin/activate"

```markdown
# ==============================================================================
# File: docs/architecture.md
# ==============================================================================
# System Architecture

This document provides a high-level overview of the Purdue Grand Prix Digital Twin system architecture.

## Component Descriptions

1.  **Data Collection (`data_collection`)**: A Python script simulates sensor data and acts as a **Kafka Producer**, publishing telemetry messages in a structured JSON format to a Kafka topic.

2.  **Message Broker (Kafka)**: **Apache Kafka** serves as the central, high-throughput message bus. It decouples the data producers from the data consumers, providing durability and scalability.

3.  **Stream Processing (`stream_processing`)**: An **Apache Spark Streaming** application consumes data from the Kafka topic in near real-time. It performs two main tasks:
    * Saves the raw telemetry data directly to a MongoDB collection for historical record-keeping.
    * Runs a real-time **Alert Generator** to check for critical conditions (e.g., high temperatures, low pressure) and writes any alerts to a separate MongoDB collection.

4.  **Data Storage (`storage`)**: **MongoDB** is used as the primary data store for its flexible document model, which maps well to the JSON sensor data. It holds three main collections:
    * `telemetry`: The raw, time-series data from the vehicle.
    * `alerts`: Critical events generated by the streaming job.
    * `analytics_results`: Summaries and insights generated by batch jobs.

5.  **Batch Analytics (`analytics`, `ml`)**: Offline **Apache Spark** batch jobs are run post-race on the historical data in MongoDB. These jobs perform complex aggregations (`DataAggregator`), analyze performance (`PerformanceAnalyzer`), and train machine learning models (`train_models`).

6.  **Web Application (`web`)**: A **Flask** application serves the front-end.
    * It provides a **RESTful API** that polls MongoDB for the latest telemetry and alerts.
    * It renders the main HTML page.
    * The user interface is a single-page application using **D3.js** to fetch data from the API and render dynamic, real-time visualizations.

```markdown
# ==============================================================================
# File: docs/api_reference.md
# ==============================================================================
# API Reference

This document provides details on the RESTful API endpoints available in the web application.

## Base URL
All API endpoints are prefixed with `/api`.

---

### Get Latest Telemetry

-   **Endpoint:** `GET /api/telemetry/latest`
-   **Description:** Retrieves the single most recent telemetry document from the database. This is used to power the live gauges on the dashboard.
-   **Success Response (200 OK):**
    ```json
    [
      {
        "_id": { "$oid": "..." },
        "session_id": "race_session_1668423600",
        "lap_id": "lap_3",
        "timestamp": { "$date": "2025-11-14T12:00:05.123Z" },
        "sensors": {
          "accelerometer": { "x": 1.25, "y": -0.5, "z": 9.81 },
          "gps": { "lat": 40.424, "lon": -86.9215, "speed": 75.5 },
          "temperature": { "engine": 96.2, "brake_fl": 480.1, "brake_fr": 475.8 },
          "tire_pressure": { "fl": 17.8, "fr": 18.1, "rl": 18.9, "rr": 19.0 },
          "engine": { "rpm": 5200 }
        }
      }
    ]
    ```
-   **Error Responses:**
    -   `404 Not Found`: If no data exists in the telemetry collection.
    -   `500 Internal Server Error`: If there is a database connection issue.

---

### Get Latest Alerts

-   **Endpoint:** `GET /api/alerts`
-   **Description:** Retrieves the 5 most recent alerts generated by the streaming pipeline.
-   **Success Response (200 OK):**
    ```json
    [
      {
        "_id": { "$oid": "..." },
        "timestamp": { "$date": "2025-11-14T12:01:15.456Z" },
        "alert_type": "Brake Temperature Warning",
        "message": "Front-left brake temperature critical: 550.7°C",
        "priority": "High",
        "session_id": "race_session_1668423600",
        "lap_id": "lap_4"
      }
    ]
    ```
-   **Error Responses:**
    -   `500 Internal Server Error`: If there is a database connection issue.

---

### Get Race History

-   **Endpoint:** `GET /api/race_history/<session_id>`
-   **Description:** Retrieves all telemetry documents for a given `session_id`, sorted by timestamp. This can be used for replaying a race or detailed analysis.
-   **URL Parameters:**
    -   `session_id` (string, required): The unique identifier for the race session.
-   **Success Response (200 OK):**
    -   Returns an array of telemetry documents, similar to the `/telemetry/latest` endpoint.
-   **Error Responses:**
    -   `404 Not Found`: If no data is found for the given `session_id`.
    -   `500 Internal Server Error`: If there is a database connection issue.

