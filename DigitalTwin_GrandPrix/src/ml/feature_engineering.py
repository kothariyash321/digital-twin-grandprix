# -*- coding: utf-8 -*-
"""feature_engineering

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Pb-6QZZzJpk5Bhhpe_WAAq2BDBr7G1-X
"""

# ==============================================================================
# File: src/ml/feature_engineering.py
# Description: Creates features and labels for ML models from raw data.
# ==============================================================================
from pyspark.sql.functions import col, when, avg, stddev
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml import Pipeline

class FeatureEngineer:
    """A class to handle feature engineering for various ML models."""

    def create_anomaly_label(self, df, feature_col, stddev_multiplier=3.0):
        """
        Creates a binary 'label' column for anomaly detection.
        An anomaly is defined as a value exceeding a number of standard deviations
        from the mean.

        Args:
            df (DataFrame): The input DataFrame.
            feature_col (str): The column to check for anomalies (e.g., 'sensors.temperature.brake_fl').
            stddev_multiplier (float): The number of standard deviations for the threshold.

        Returns:
            DataFrame: The DataFrame with an added 'label' column (1 for anomaly, 0 for normal).
        """
        print(f"Creating anomaly label for column: {feature_col}")

        stats = df.select(
            avg(col(feature_col)).alias("avg_val"),
            stddev(col(feature_col)).alias("stddev_val")
        ).first()

        avg_val = stats['avg_val']
        stddev_val = stats['stddev_val']
        anomaly_threshold = avg_val + (stddev_multiplier * stddev_val)

        print(f"Stats for {feature_col}: Avg={avg_val:.2f}, StdDev={stddev_val:.2f}")
        print(f"Anomaly threshold set to > {anomaly_threshold:.2f}")

        labeled_df = df.withColumn(
            "label",
            when(col(feature_col) > anomaly_threshold, 1.0).otherwise(0.0)
        )
        return labeled_df

    def create_feature_pipeline(self, input_cols):
        """
        Creates a Spark ML Pipeline for assembling and scaling features.

        Args:
            input_cols (list): A list of column names to be used as features.

        Returns:
            Pipeline: A Spark ML Pipeline object.
        """
        print(f"Creating feature pipeline for columns: {input_cols}")

        # Step 1: Assemble features into a single vector
        assembler = VectorAssembler(inputCols=input_cols, outputCol="raw_features", handleInvalid="skip")

        # Step 2: Scale features to have zero mean and unit variance
        scaler = StandardScaler(inputCol="raw_features", outputCol="features")

        pipeline = Pipeline(stages=[assembler, scaler])
        return pipeline