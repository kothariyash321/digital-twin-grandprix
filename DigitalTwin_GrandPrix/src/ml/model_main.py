# -*- coding: utf-8 -*-
"""model_main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iC756RGDmn7VvAId0QtLgA5WiFKTGAQv
"""

def main():
    """Main function to run the ML model training job."""
    # Load configurations
    spark_config = load_config("src/config/spark.yaml")
    mongo_config = load_config("src/config/mongodb.yaml")

    # Initialize Spark
    spark = initialize_spark_session(spark_config, mongo_config)
    spark.sparkContext.setLogLevel(spark_config['log_level'])

    # 1. Load data
    print("Loading historical telemetry data...")
    df = spark.read.format("mongo").load().na.drop()
    df.cache()

    # --- Predictive Maintenance Model: Brake Failure Prediction ---
    print("\n--- Training Predictive Maintenance Model for Brakes ---")

    # 2. Initialize ML components
    feature_engineer = FeatureEngineer()
    model_evaluator = ModelEvaluator()

    # 3. Feature Engineering
    labeled_df = feature_engineer.create_anomaly_label(df, 'sensors.temperature.brake_fl')

    feature_cols = [
        "sensors.engine.rpm",
        "sensors.gps.speed",
        "sensors.accelerometer.x" # Longitudinal G-force (braking/acceleration)
    ]
    feature_pipeline = feature_engineer.create_feature_pipeline(feature_cols)

    # 4. Define the Model
    rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=100, seed=42)

    # 5. Create the full training pipeline
    training_pipeline = Pipeline(stages=[feature_pipeline, rf])

    # 6. Train and Evaluate
    (trainingData, testData) = labeled_df.randomSplit([0.8, 0.2], seed=42)

    print("\nTraining the RandomForest model...")
    model = training_pipeline.fit(trainingData)

    print("\nEvaluating model on test data...")
    predictions = model.transform(testData)
    model_evaluator.evaluate(predictions)

    # 7. Save the Model
    model_collection = mongo_config['collections']['ml_models']
    model_path = f"/tmp/purdue_gp_models/{model_collection}/brake_anomaly_rf"
    model.write().overwrite().save(model_path)
    print(f"\nModel successfully saved to: {model_path}")
    print("This model can be loaded by other jobs for inference.")

    df.unpersist()
    spark.stop()

if __name__ == "__main__":
    main()