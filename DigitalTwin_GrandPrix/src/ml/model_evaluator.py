# -*- coding: utf-8 -*-
"""model_evaluator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1etqh-nMm0dH5mkyQ318kqwPRn0_xGqZf
"""

```python
# ==============================================================================
# File: src/ml/model_evaluator.py
# Description: Evaluates the performance of trained ML models.
# ==============================================================================
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

class ModelEvaluator:
    """A class to evaluate classification models."""

    def __init__(self, label_col="label", prediction_col="prediction"):
        """Initializes the evaluator with specified columns."""
        self.label_col = label_col
        self.prediction_col = prediction_col

    def evaluate(self, predictions_df):
        """
        Calculates and returns key performance metrics for a classification model.

        Args:
            predictions_df (DataFrame): DataFrame containing predictions and true labels.

        Returns:
            dict: A dictionary containing metrics like accuracy, F1-score, etc.
        """
        print("Evaluating model performance...")

        # Accuracy
        accuracy_evaluator = MulticlassClassificationEvaluator(
            labelCol=self.label_col,
            predictionCol=self.prediction_col,
            metricName="accuracy"
        )
        accuracy = accuracy_evaluator.evaluate(predictions_df)

        # F1 Score
        f1_evaluator = MulticlassClassificationEvaluator(
            labelCol=self.label_col,
            predictionCol=self.prediction_col,
            metricName="f1"
        )
        f1_score = f1_evaluator.evaluate(predictions_df)

        # Weighted Precision
        precision_evaluator = MulticlassClassificationEvaluator(
            labelCol=self.label_col,
            predictionCol=self.prediction_col,
            metricName="weightedPrecision"
        )
        precision = precision_evaluator.evaluate(predictions_df)

        metrics = {
            "accuracy": accuracy,
            "f1_score": f1_score,
            "precision": precision
        }

        print(f"  - Accuracy: {accuracy:.2%}")
        print(f"  - F1-Score: {f1_score:.2%}")
        print(f"  - Precision: {precision:.2%}")

        return metrics