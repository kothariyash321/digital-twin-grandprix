# -*- coding: utf-8 -*-
"""ml/train_models.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hqPUT5JWllO54nNExw_c5F1WrEnkz3eF
"""

# ==============================================================================
# File: ml/train_models.py
# Description: Trains machine learning models for predictive maintenance and
#              other analytical tasks using historical data from MongoDB.
# ==============================================================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, stddev, when
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# --- Configuration ---
MONGO_URI = "mongodb://localhost:27017/"
MONGO_DB = "grand_prix_db"

# --- Spark Session Initialization ---
def initialize_spark_session():
    """Initializes a SparkSession for ML training with MongoDB."""
    return SparkSession \
        .builder \
        .appName("GrandPrixModelTraining") \
        .config("spark.mongodb.input.uri", f"{MONGO_URI}{MONGO_DB}.telemetry") \
        .config("spark.mongodb.output.uri", f"{MONGO_URI}{MONGO_DB}.ml_models") \
        .getOrCreate()

# --- Main Model Training Logic ---
if __name__ == "__main__":
    spark = initialize_spark_session()
    spark.sparkContext.setLogLevel("ERROR")

    # 1. Load historical telemetry data from MongoDB
    print("Loading historical data from MongoDB...")
    df = spark.read.format("mongo").load().na.drop()

    # --- Predictive Maintenance Model: Brake Failure Prediction ---
    print("\n--- Training Predictive Maintenance Model for Brakes ---")

    # 2. Feature Engineering & Label Creation
    # We'll define an "anomaly" or "potential failure" event as the brake
    # temperature exceeding a certain threshold (e.g., 3 standard deviations above the mean).

    # Calculate mean and stddev for front-left brake temperature
    stats = df.select(
        avg(col("sensors.temperature.brake_fl")).alias("avg_temp"),
        stddev(col("sensors.temperature.brake_fl")).alias("stddev_temp")
    ).first()

    avg_temp = stats['avg_temp']
    stddev_temp = stats['stddev_temp']
    anomaly_threshold = avg_temp + (3 * stddev_temp)

    print(f"Brake Temperature Stats: Avg={avg_temp:.2f}, StdDev={stddev_temp:.2f}")
    print(f"Anomaly Threshold set to > {anomaly_threshold:.2f}Â°C")

    # Create a label column: 1 for anomaly, 0 for normal
    featured_df = df.withColumn("label",
        when(col("sensors.temperature.brake_fl") > anomaly_threshold, 1.0).otherwise(0.0)
    )

    # Select features that might predict a brake temperature spike
    # For example: engine RPM, speed, and accelerometer data (as a proxy for hard braking)
    feature_cols = [
        "sensors.engine.rpm",
        "sensors.gps.speed",
        "sensors.accelerometer.x" # Longitudinal G-force (braking/acceleration)
    ]

    # 3. Create a ML Pipeline
    # The pipeline will assemble features, scale them, and train a classifier.

    # Step 1: Assemble features into a single vector
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="raw_features")

    # Step 2: Scale features to have zero mean and unit variance
    scaler = StandardScaler(inputCol="raw_features", outputCol="features")

    # Step 3: Define the classification model
    # RandomForest is a good choice for its robustness.
    rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=100)

    # Chain the steps into a pipeline
    pipeline = Pipeline(stages=[assembler, scaler, rf])

    # 4. Train and Evaluate the Model
    print("Splitting data and training the model...")
    (trainingData, testData) = featured_df.randomSplit([0.8, 0.2], seed=42)

    model = pipeline.fit(trainingData)

    print("Evaluating model on test data...")
    predictions = model.transform(testData)

    # Use an evaluator to check accuracy, F1-score, etc.
    evaluator = MulticlassClassificationEvaluator(
        labelCol="label",
        predictionCol="prediction",
        metricName="accuracy"
    )
    accuracy = evaluator.evaluate(predictions)

    f1_evaluator = MulticlassClassificationEvaluator(
        labelCol="label",
        predictionCol="prediction",
        metricName="f1"
    )
    f1_score = f1_evaluator.evaluate(predictions)

    print(f"\nModel Performance:")
    print(f"  Accuracy: {accuracy:.2%}")
    print(f"  F1-Score: {f1_score:.2%}")

    # Show some predictions
    print("\nSample Predictions (1 = Anomaly Predicted):")
    predictions.select("label", "prediction", "sensors.temperature.brake_fl").show(10)

    # 5. Save the Trained Model
    # The model can be saved to be loaded later by the streaming job for real-time inference.
    model_path = "/tmp/purdue_gp_brake_model"
    model.write().overwrite().save(model_path)
    print(f"\nPredictive maintenance model saved to: {model_path}")
    print("This model can now be loaded in the streaming job to make real-time predictions.")

    spark.stop()