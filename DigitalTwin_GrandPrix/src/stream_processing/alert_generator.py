# -*- coding: utf-8 -*-
"""alert_generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E3y2W20ZCi6nnOLjIhOc817BzZmv_CFH
"""

# ==============================================================================
# File: src/stream_processing/alert_generator.py
# Description: Contains business logic for generating alerts from sensor data.
#              This module is independent of Spark.
# ==============================================================================
import json

class AlertGenerator:
    """
    A class to generate alerts based on predefined rules for sensor data.
    """
    def __init__(self):
        # Thresholds could be loaded from a config file for more flexibility
        self.thresholds = {
            'brake_temp_critical': 500,  # degrees Celsius
            'tire_pressure_low': 15,     # PSI
            'tire_pressure_high': 22,    # PSI
            'engine_rpm_critical': 7000, # RPM
        }

    def check_telemetry(self, sensors_json_string):
        """
        Checks a JSON string of sensor data for any alert conditions.

        Args:
            sensors_json_string (str): A JSON string representing the 'sensors' object.

        Returns:
            str: A JSON string of a list of alerts, or None if no alerts.
        """
        if not sensors_json_string:
            return None

        sensors = json.loads(sensors_json_string)
        alerts = []

        # 1. Brake Temperature Check
        temp_sensors = sensors.get('temperature', {})
        if temp_sensors.get('brake_fl', 0) > self.thresholds['brake_temp_critical']:
            alerts.append(self._create_alert(
                "Brake Temperature Critical",
                f"Front-left brake temp exceeded threshold: {temp_sensors['brake_fl']:.1f}Â°C",
                "High"
            ))

        # 2. Tire Pressure Check
        tpms_sensors = sensors.get('tire_pressure', {})
        for tire, pressure in tpms_sensors.items():
            if pressure < self.thresholds['tire_pressure_low']:
                alerts.append(self._create_alert(
                    "Tire Pressure Low",
                    f"Tire {tire.upper()} pressure is low: {pressure:.1f} PSI",
                    "Medium"
                ))

        # 3. Engine RPM Check
        engine_sensors = sensors.get('engine', {})
        if engine_sensors.get('rpm', 0) > self.thresholds['engine_rpm_critical']:
            alerts.append(self._create_alert(
                "Engine RPM Critical",
                f"Engine RPM exceeded threshold: {engine_sensors['rpm']:.0f} RPM",
                "High"
            ))

        return json.dumps(alerts) if alerts else None

    def _create_alert(self, alert_type, message, priority):
        """Helper to create a structured alert dictionary."""
        return {
            "alert_type": alert_type,
            "message": message,
            "priority": priority
        }

```python
# ==============================================================================
# File: src/stream_processing/kafka_consumer.py
# Description: Handles reading and deserializing data from a Kafka topic.
# ==============================================================================
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType

def get_telemetry_schema():
    """Defines and returns the schema for the incoming JSON sensor data."""
    return StructType([
        StructField("timestamp", TimestampType(), True),
        StructField("session_id", StringType(), True),
        StructField("lap_id", StringType(), True),
        StructField("sensors", StructType([
            StructField("accelerometer", StructType([
                StructField("x", DoubleType()), StructField("y", DoubleType()), StructField("z", DoubleType())
            ])),
            StructField("gps", StructType([
                StructField("lat", DoubleType()), StructField("lon", DoubleType()), StructField("speed", DoubleType())
            ])),
            StructField("temperature", StructType([
                StructField("engine", DoubleType()), StructField("brake_fl", DoubleType()), StructField("brake_fr", DoubleType())
            ])),
            StructField("tire_pressure", StructType([
                StructField("fl", DoubleType()), StructField("fr", DoubleType()), StructField("rl", DoubleType()), StructField("rr", DoubleType())
            ])),
            StructField("engine", StructType([
                StructField("rpm", DoubleType())
            ]))
        ]))
    ])

def create_kafka_stream(spark, kafka_config):
    """
    Creates a Spark streaming DataFrame connected to a Kafka topic.

    Args:
        spark (SparkSession): The active Spark session.
        kafka_config (dict): Configuration for the Kafka connection.

    Returns:
        DataFrame: A streaming DataFrame with the deserialized telemetry data.
    """
    schema = get_telemetry_schema()

    # Read from Kafka source
    kafka_df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", kafka_config['broker']) \
        .option("subscribe", kafka_config['topic']) \
        .option("startingOffsets", "latest") \
        .load()

    # Deserialize JSON data and apply schema
    telemetry_stream = kafka_df \
        .select(from_json(col("value").cast("string"), schema).alias("data")) \
        .select("data.*")

    return telemetry_stream

```python
# ==============================================================================
# File: src/stream_processing/spark_streaming.py
# Description: Main Spark Streaming application that orchestrates the pipeline.
# ==============================================================================
import yaml
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col, to_json, from_json, explode, current_timestamp
from pyspark.sql.types import StringType, ArrayType, StructType, StructField

from kafka_consumer import create_kafka_stream
from alert_generator import AlertGenerator

def load_config(path):
    """Loads a YAML configuration file."""
    with open(path, 'r') as f:
        return yaml.safe_load(f)

def initialize_spark_session(spark_config, mongo_config):
    """Initializes and returns a SparkSession."""
    mongo_uri = mongo_config['uri']
    mongo_db = mongo_config['database']

    return SparkSession \
        .builder \
        .appName(spark_config['app_name']) \
        .config("spark.jars.packages", ",".join(spark_config['packages'])) \
        .config("spark.mongodb.output.uri", f"{mongo_uri}{mongo_db}") \
        .getOrCreate()

def main():
    """Main function to run the streaming application."""
    # Load configurations
    spark_config = load_config("src/config/spark.yaml")
    kafka_config = load_config("src/config/kafka.yaml")
    mongo_config = load_config("src/config/mongodb.yaml")

    # Initialize Spark
    spark = initialize_spark_session(spark_config, mongo_config)
    spark.sparkContext.setLogLevel(spark_config['log_level'])

    # 1. Create the Kafka stream
    telemetry_stream = create_kafka_stream(spark, kafka_config)

    # 2. Set up Alert Generation
    alert_generator = AlertGenerator()
    check_alerts_udf = udf(alert_generator.check_telemetry, StringType())

    # Add a column with alerts by applying the UDF
    stream_with_alerts = telemetry_stream \
        .withColumn("sensors_json", to_json(col("sensors"))) \
        .withColumn("alerts", check_alerts_udf(col("sensors_json")))

    # 3. Define Stream Sinks (Outputs)

    # Sink 1: Write raw telemetry to MongoDB
    raw_telemetry_writer = stream_with_alerts \
        .select("timestamp", "session_id", "lap_id", "sensors") \
        .writeStream \
        .foreachBatch(lambda df, epoch_id: df.write.format("mongo").mode("append").option("collection", mongo_config['collections']['telemetry']).save()) \
        .outputMode("update")

    # Sink 2: Filter, process, and write alerts to MongoDB
    alert_schema = ArrayType(StructType([
        StructField("alert_type", StringType()),
        StructField("message", StringType()),
        StructField("priority", StringType())
    ]))

    alerts_writer = stream_with_alerts \
        .filter(col("alerts").isNotNull()) \
        .select(
            col("session_id"),
            col("lap_id"),
            explode(from_json(col("alerts"), alert_schema)).alias("alert")
        ) \
        .select("session_id", "lap_id", "alert.*", current_timestamp().alias("timestamp")) \
        .writeStream \
        .foreachBatch(lambda df, epoch_id: df.write.format("mongo").mode("append").option("collection", mongo_config['collections']['alerts']).save()) \
        .outputMode("update")

    # 4. Start the streams
    print("Starting streaming queries...")
    raw_query = raw_telemetry_writer.start()
    alerts_query = alerts_writer.start()

    raw_query.awaitTermination()
    alerts_query.awaitTermination()

if __name__ == "__main__":
    main()