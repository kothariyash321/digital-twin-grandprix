# -*- coding: utf-8 -*-
"""mongodb_client

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uplCYZUUUyR18Yk8uJfD5YY2ipc5sFSX
"""

# ==============================================================================
# File: src/storage/mongodb_client.py
# Description: A utility client for interacting with MongoDB. Useful for
#              scripts, testing, or direct data access outside of Spark/Flask.
# ==============================================================================
import yaml
from pymongo import MongoClient

class MongoDBClient:
    """A client for connecting to and interacting with MongoDB."""

    def __init__(self, config_path='src/config/mongodb.yaml'):
        """
        Initializes the client and connects to the database.

        Args:
            config_path (str): Path to the MongoDB YAML configuration file.
        """
        self.config = self._load_config(config_path)
        try:
            self.client = MongoClient(self.config['uri'], serverSelectionTimeoutMS=5000)
            # The ismaster command is cheap and does not require auth.
            self.client.admin.command('ismaster')
            self.db = self.client[self.config['database']]
            print("Successfully connected to MongoDB.")
        except Exception as e:
            print(f"Error connecting to MongoDB: {e}")
            self.client = None
            self.db = None

    def _load_config(self, path):
        """Loads the YAML configuration file."""
        with open(path, 'r') as f:
            return yaml.safe_load(f)

    def get_collection(self, collection_name_key):
        """
        Gets a collection object from the database.

        Args:
            collection_name_key (str): The key for the collection name in the config file
                                       (e.g., 'telemetry', 'alerts').

        Returns:
            Collection: A PyMongo Collection object, or None if connection failed.
        """
        if not self.db:
            return None
        collection_name = self.config['collections'].get(collection_name_key)
        if not collection_name:
            raise ValueError(f"Collection key '{collection_name_key}' not found in config.")
        return self.db[collection_name]

    def find_one(self, collection_key, query={}):
        """Finds a single document in a collection."""
        collection = self.get_collection(collection_key)
        return collection.find_one(query) if collection else None

    def find(self, collection_key, query={}):
        """Finds multiple documents in a collection."""
        collection = self.get_collection(collection_key)
        return collection.find(query) if collection else []

# Example usage:
if __name__ == '__main__':
    mongo_client = MongoDBClient()
    if mongo_client.db:
        print("\nFetching latest telemetry record:")
        latest_telemetry = mongo_client.find_one('telemetry', {})
        print(latest_telemetry)

        print("\nFetching latest alert:")
        latest_alert = mongo_client.find_one('alerts', {})
        print(latest_alert)

```python
# ==============================================================================
# File: src/storage/data_schema.py
# Description: Defines the document structure for collections in MongoDB.
#              This file is for documentation and clarity.
# ==============================================================================

# --- Telemetry Collection Schema ---
# Stores the raw and processed sensor data from each race session.
# Collection name is defined in config: collections.telemetry

telemetry_document_example = {
    "_id": "ObjectId('...')",
    "session_id": "race_session_1668423600",
    "lap_id": "lap_3",
    "timestamp": "ISODate('2025-11-14T12:00:05.123Z')",
    "sensors": {
        "accelerometer": {"x": 1.25, "y": -0.5, "z": 9.81},
        "gps": {"lat": 40.4240, "lon": -86.9215, "speed": 75.5},
        "temperature": {"engine": 96.2, "brake_fl": 480.1, "brake_fr": 475.8},
        "tire_pressure": {"fl": 17.8, "fr": 18.1, "rl": 18.9, "rr": 19.0},
        "engine": {"rpm": 5200}
    }
}

# --- Alerts Collection Schema ---
# Stores alerts generated by the real-time streaming analytics job.
# Collection name is defined in config: collections.alerts

alert_document_example = {
    "_id": "ObjectId('...')",
    "timestamp": "ISODate('2025-11-14T12:01:15.456Z')",
    "alert_type": "Brake Temperature Warning",
    "message": "Front-left brake temperature critical: 550.7Â°C",
    "priority": "High",
    "session_id": "race_session_1668423600",
    "lap_id": "lap_4"
}

# --- Analytics Results Collection Schema ---
# Stores results from post-race batch analytics jobs.
# Collection name is defined in config: collections.analytics_results

lap_summary_result_example = {
    "_id": "ObjectId('...')",
    "session_id": "race_session_1668423600",
    "lap_id": "lap_3",
    "lap_time_seconds": 92.45,
    "avg_speed_kmh": 72.8,
    "max_engine_temp": 98.5,
    "min_rpm": 3500,
    "max_rpm": 6800
}

```python
# ==============================================================================
# File: src/storage/elastic_client.py
# Description: Placeholder for an Elasticsearch client.
# ==============================================================================

# This is a placeholder to match the specified architecture.
# A full implementation would use the 'elasticsearch' library.
# Example: from elasticsearch import Elasticsearch

class ElasticClient:
    def __init__(self, hosts=["http://localhost:9200"]):
        print("Elasticsearch client placeholder initialized.")
        # self.client = Elasticsearch(hosts)

    def index_document(self, index_name, document):
        print(f"Placeholder: Indexing document into '{index_name}'.")
        # self.client.index(index=index_name, document=document)

    def search(self, index_name, query):
        print(f"Placeholder: Searching '{index_name}' with query.")
        # return self.client.search(index=index_name, body=query)

"""### 3. Batch Analytics (`src/analytics/`)

This directory contains the logic for post-race analysis, broken down into modular components.
"""

# ==============================================================================
# File: src/analytics/data_aggregator.py
# Description: Performs data aggregations on the raw telemetry data.
# ==============================================================================
from pyspark.sql.functions import col, min, max, avg, count

class DataAggregator:
    """A class to perform common data aggregations on telemetry DataFrames."""

    def calculate_lap_summary(self, telemetry_df):
        """
        Calculates a summary for each lap in the telemetry data.

        Args:
            telemetry_df (DataFrame): The input DataFrame with raw telemetry.

        Returns:
            DataFrame: A DataFrame with one row per lap, containing summary stats.
        """
        print("Aggregating data to create lap summaries...")

        lap_summary_df = telemetry_df.groupBy("session_id", "lap_id") \
            .agg(
                (max(col("timestamp")).cast("long") - min(col("timestamp")).cast("long")).alias("lap_time_seconds"),
                avg("sensors.gps.speed").alias("avg_speed_kmh"),
                max("sensors.gps.speed").alias("max_speed_kmh"),
                max("sensors.temperature.engine").alias("max_engine_temp"),
                min("sensors.engine.rpm").alias("min_rpm"),
                max("sensors.engine.rpm").alias("max_rpm"),
                count("*").alias("data_points")
            ) \
            .orderBy(col("lap_time_seconds").asc())

        return lap_summary_df

```python
# ==============================================================================
# File: src/analytics/performance_analyzer.py
# Description: Performs higher-level performance analysis.
# ==============================================================================
from pyspark.sql.functions import col, lead, lag
from pyspark.sql.window import Window

class PerformanceAnalyzer:
    """A class for advanced performance analysis."""

    def find_fastest_lap(self, lap_summary_df):
        """Finds the fastest lap from a lap summary DataFrame."""
        print("Finding the fastest overall lap...")
        fastest_lap = lap_summary_df.orderBy(col("lap_time_seconds").asc()).first()
        return fastest_lap

    def analyze_racing_line(self, telemetry_df, lap_id):
        """
        Extracts GPS data for a specific lap to analyze the racing line.

        Args:
            telemetry_df (DataFrame): The raw telemetry DataFrame.
            lap_id (str): The specific lap_id to analyze.

        Returns:
            DataFrame: A DataFrame with timestamp, lat, lon, and speed for the given lap.
        """
        print(f"Extracting racing line data for lap: {lap_id}")
        racing_line_df = telemetry_df \
            .filter(col("lap_id") == lap_id) \
            .select(
                col("timestamp"),
                col("sensors.gps.lat").alias("latitude"),
                col("sensors.gps.lon").alias("longitude"),
                col("sensors.gps.speed").alias("speed")
            ) \
            .orderBy(col("timestamp").asc())

        return racing_line_df

```python
# ==============================================================================
# File: src/analytics/batch_processor.py
# Description: Main batch processing job to run analytics on historical data.
# ==============================================================================
import yaml
from pyspark.sql import SparkSession

from data_aggregator import DataAggregator
from performance_analyzer import PerformanceAnalyzer

def load_config(path):
    with open(path, 'r') as f:
        return yaml.safe_load(f)

def initialize_spark_session(spark_config, mongo_config):
    mongo_uri = mongo_config['uri']
    mongo_db = mongo_config['database']

    return SparkSession \
        .builder \
        .appName(f"{spark_config['app_name']}-Batch") \
        .config("spark.jars.packages", ",".join(spark_config['packages'])) \
        .config("spark.mongodb.input.uri", f"{mongo_uri}{mongo_db}.{mongo_config['collections']['telemetry']}") \
        .config("spark.mongodb.output.uri", f"{mongo_uri}{mongo_db}.{mongo_config['collections']['analytics_results']}") \
        .getOrCreate()

def main():
    """Main function to run the batch analytics job."""
    # Load configurations
    spark_config = load_config("src/config/spark.yaml")
    mongo_config = load_config("src/config/mongodb.yaml")

    # Initialize Spark
    spark = initialize_spark_session(spark_config, mongo_config)
    spark.sparkContext.setLogLevel(spark_config['log_level'])

    # 1. Load historical data from MongoDB
    print("Loading historical telemetry data from MongoDB...")
    telemetry_df = spark.read.format("mongo").load()
    telemetry_df.cache() # Cache for faster access
    print(f"Loaded {telemetry_df.count()} records.")

    # 2. Initialize analysis components
    aggregator = DataAggregator()
    analyzer = PerformanceAnalyzer()

    # 3. Perform aggregations
    lap_summary_df = aggregator.calculate_lap_summary(telemetry_df)
    print("\nLap Summary Results:")
    lap_summary_df.show(10, truncate=False)

    # 4. Perform higher-level analysis
    fastest_lap = analyzer.find_fastest_lap(lap_summary_df)
    if fastest_lap:
        print(f"\nFastest Lap Found: {fastest_lap['lap_id']} with time {fastest_lap['lap_time_seconds']:.2f}s")

        # Analyze the racing line of the fastest lap
        racing_line_df = analyzer.analyze_racing_line(telemetry_df, fastest_lap['lap_id'])
        print(f"\nRacing Line Data for Fastest Lap ({fastest_lap['lap_id']}):")
        racing_line_df.show(10, truncate=False)

    # 5. Save results to MongoDB
    print("\nSaving lap summary results to MongoDB...")
    results_collection = mongo_config['collections']['analytics_results']
    lap_summary_df.write.format("mongo").mode("overwrite").option("collection", results_collection).save()
    print(f"Results saved to collection: '{results_collection}'")

    telemetry_df.unpersist()
    spark.stop()

if __name__ == "__main__":
    main()