# -*- coding: utf-8 -*-
"""Batch_Analytics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGv5k2PDynhlCZJo13fm8bPg4Zjma2wI
"""

# ==============================================================================
# File: batch_analytics.py
# Description: Spark batch job to run post-race analytics on historical data
#              stored in MongoDB.
# ==============================================================================

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator

def initialize_batch_spark_session():
    """Initializes a SparkSession for batch processing with MongoDB."""
    return SparkSession \
        .builder \
        .appName("GrandPrixBatchAnalytics") \
        .config("spark.mongodb.input.uri", f"{MONGO_URI}{MONGO_DB}.telemetry") \
        .config("spark.mongodb.output.uri", f"{MONGO_URI}{MONGO_DB}.analytics_results") \
        .getOrCreate()

if __name__ == "__main__":
    spark = initialize_batch_spark_session()
    spark.sparkContext.setLogLevel("ERROR")

    # 1. Load historical data from MongoDB
    df = spark.read.format("mongo").load()
    df.printSchema()
    df.show(5)

    # --- Analytics Example 1: Lap Time Analysis ---
    print("Performing Lap Time Analysis...")
    lap_summary = df.groupBy("session_id", "lap_id") \
        .agg(
            (max(col("timestamp")).cast("long") - min(col("timestamp")).cast("long")).alias("lap_time_seconds"),
            avg("sensors.gps.speed").alias("avg_speed_kmh"),
            max("sensors.temperature.engine").alias("max_engine_temp")
        ) \
        .orderBy("lap_time_seconds")

    print("Top 5 Fastest Laps:")
    lap_summary.show(5)

    # Save results to a new collection
    lap_summary.write.format("mongo").mode("overwrite").option("collection", "lap_summary_results").save()
    print("Lap summary results saved to MongoDB.")


    # --- Analytics Example 2: Predictive Model for Lap Time ---
    print("\nTraining a GBT model to predict lap times...")

    # Feature Engineering: Create a feature vector
    # Let's predict lap time based on avg speed and max engine temp
    feature_df = lap_summary.select(
        col("lap_time_seconds").alias("label"),
        col("avg_speed_kmh"),
        col("max_engine_temp")
    ).na.drop()

    assembler = VectorAssembler(
        inputCols=["avg_speed_kmh", "max_engine_temp"],
        outputCol="features"
    )

    model_df = assembler.transform(feature_df)

    # Split data and train model
    (trainingData, testData) = model_df.randomSplit([0.8, 0.2], seed=1234)

    gbt = GBTRegressor(featuresCol="features", labelCol="label", maxIter=10)
    model = gbt.fit(trainingData)

    # Make predictions and evaluate
    predictions = model.transform(testData)
    print("Model Predictions:")
    predictions.select("label", "prediction").show(5)

    evaluator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
    rmse = evaluator.evaluate(predictions)
    print(f"Root Mean Squared Error (RMSE) on test data = {rmse}")

    # You could save this model for later use
    # model.write().overwrite().save("/path/to/models/gbt_lap_time_model")
    # print("Model saved.")

    spark.stop()