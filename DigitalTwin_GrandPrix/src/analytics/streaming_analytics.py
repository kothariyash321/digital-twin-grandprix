# -*- coding: utf-8 -*-
"""Streaming_Analytics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ql87Xy_zgNpczSjXcBGIxH0sphZqS8CR
"""

# ==============================================================================
# File: streaming_analytics.py
# Description: Spark Streaming application to process real-time telemetry from
#              Kafka, run live analytics/ML models, and store results in MongoDB.
# ==============================================================================

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, udf
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType
import json
from datetime import datetime

# --- Configuration ---
# Ensure you have the correct Spark-Kafka and MongoDB connector packages.
# Example for spark-submit:
# --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1

KAFKA_TOPIC = "kart-telemetry"
KAFKA_SERVER = "localhost:9092"
MONGO_URI = "mongodb://localhost:27017/"
MONGO_DB = "grand_prix_db"

# --- Spark Session Initialization ---
def initialize_spark_session():
    """Initializes and returns a SparkSession with MongoDB configuration."""
    return SparkSession \
        .builder \
        .appName("GrandPrixStreamingAnalytics") \
        .config("spark.mongodb.output.uri", f"{MONGO_URI}{MONGO_DB}.telemetry") \
        .config("spark.mongodb.output.database", MONGO_DB) \
        .getOrCreate()

# --- Data Schema Definition ---
def get_telemetry_schema():
    """Defines the schema for the incoming JSON sensor data."""
    return StructType([
        StructField("timestamp", TimestampType(), True),
        StructField("session_id", StringType(), True),
        StructField("lap_id", StringType(), True),
        StructField("sensors", StructType([
            StructField("accelerometer", StructType([
                StructField("x", DoubleType(), True),
                StructField("y", DoubleType(), True),
                StructField("z", DoubleType(), True)
            ]), True),
            StructField("gps", StructType([
                StructField("lat", DoubleType(), True),
                StructField("lon", DoubleType(), True),
                StructField("speed", DoubleType(), True)
            ]), True),
            StructField("temperature", StructType([
                StructField("engine", DoubleType(), True),
                StructField("brake_fl", DoubleType(), True),
                StructField("brake_fr", DoubleType(), True)
            ]), True),
            StructField("tire_pressure", StructType([
                StructField("fl", DoubleType(), True),
                StructField("fr", DoubleType(), True),
                StructField("rl", DoubleType(), True),
                StructField("rr", DoubleType(), True)
            ]), True),
             StructField("engine", StructType([
                StructField("rpm", DoubleType(), True)
            ]), True)
        ]), True)
    ])

# --- User Defined Functions (UDFs) for Analytics ---

def check_for_alerts(sensors_json):
    """
    UDF to check for safety-critical conditions in real-time.
    This function processes a JSON string of sensor data.
    """
    sensors = json.loads(sensors_json)
    alerts = []

    # Example 1: Brake Temperature Alert
    if sensors.get("temperature", {}).get("brake_fl", 0) > 500:
        alerts.append({
            "alert_type": "Brake Temperature Warning",
            "message": f"Front-left brake temperature critical: {sensors['temperature']['brake_fl']}Â°C",
            "priority": "High"
        })

    # Example 2: Tire Pressure Alert
    if sensors.get("tire_pressure", {}).get("fl", 20) < 15:
        alerts.append({
            "alert_type": "Tire Pressure Warning",
            "message": f"Front-left tire pressure low: {sensors['tire_pressure']['fl']} PSI",
            "priority": "Medium"
        })

    return json.dumps(alerts) if alerts else None

# Register the UDF
check_alerts_udf = udf(check_for_alerts, StringType())

# --- Main Streaming Logic ---
if __name__ == "__main__":
    spark = initialize_spark_session()
    spark.sparkContext.setLogLevel("ERROR") # Reduce verbosity

    schema = get_telemetry_schema()

    # 1. Read from Kafka source
    kafka_df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_SERVER) \
        .option("subscribe", KAFKA_TOPIC) \
        .option("startingOffsets", "latest") \
        .load()

    # 2. Deserialize JSON data and apply schema
    telemetry_df = kafka_df.select(from_json(col("value").cast("string"), schema).alias("data")).select("data.*")

    # 3. Process the stream: Save raw data and generate alerts

    # Sink 1: Write raw telemetry to MongoDB
    # This provides the complete historical record for batch analysis
    raw_telemetry_query = telemetry_df \
        .writeStream \
        .foreachBatch(lambda df, epoch_id: df.write.format("mongo").mode("append").option("collection", "telemetry").save()) \
        .start()

    # 4. Generate and save alerts
    # Apply the UDF to create an 'alerts' column
    alerts_df = telemetry_df \
        .withColumn("sensors_json", to_json(col("sensors"))) \
        .withColumn("alerts", check_alerts_udf(col("sensors_json"))) \
        .filter(col("alerts").isNotNull())

    # Sink 2: Write alerts to a separate MongoDB collection
    alerts_query = alerts_df \
        .select(from_json(col("alerts"), ArrayType(StructType([ # Define alert schema
            StructField("alert_type", StringType()),
            StructField("message", StringType()),
            StructField("priority", StringType())
        ]))).alias("alert_data")) \
        .select(explode(col("alert_data")).alias("alert")) \
        .select("alert.*", current_timestamp().alias("timestamp")) \
        .writeStream \
        .foreachBatch(lambda df, epoch_id: df.write.format("mongo").mode("append").option("collection", "alerts").save()) \
        .start()


    print("Streaming queries started. Waiting for termination...")
    spark.streams.awaitAnyTermination()